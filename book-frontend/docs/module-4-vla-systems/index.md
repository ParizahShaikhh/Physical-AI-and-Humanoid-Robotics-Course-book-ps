# Module 4: Vision-Language-Action (VLA)

Welcome to Module 4 of the Physical AI and Humanoid Robotics Course. This module covers the complete Vision-Language-Action pipeline, integrating speech processing, natural language understanding, and robot action execution to enable humanoid robots to understand natural language commands and execute multi-step physical tasks.

## Chapters

This module is organized into 7 comprehensive chapters:

1. [Vision-Language-Action in Physical AI](./chapter-1-vla-physical-ai.md) - Fundamentals of the VLA paradigm
2. [Speech-to-Text with OpenAI Whisper](./chapter-2-speech-to-text.md) - Converting speech to structured commands
3. [Language Understanding and Task Decomposition](./chapter-3-language-understanding.md) - Parsing and decomposing commands
4. [LLM-Based Cognitive Planning](./chapter-4-llm-planning.md) - High-level planning with LLMs
5. [Vision-Guided Action and Object Interaction](./chapter-5-vision-action.md) - Visual perception and interaction
6. [Orchestrating ROS 2 Actions for Autonomy](./chapter-6-ros2-orchestration.md) - ROS 2 integration
7. [Capstone: The Autonomous Humanoid](./chapter-7-capstone.md) - Complete system integration

## Learning Objectives

By the end of this module, you will be able to:
- Explain the Vision-Language-Action paradigm in Physical AI
- Integrate speech-to-text processing using OpenAI Whisper
- Implement language understanding and task decomposition systems
- Use LLMs for cognitive planning and decision making
- Combine vision and action for object interaction
- Orchestrate ROS 2 actions for autonomous behavior
- Build a complete autonomous humanoid system

## Next Module

After completing this module, continue with [Module 5: Deployment, Integration, and Real-World Humanoids](../module-5-real-world-humanoids/index.md) to learn about deploying humanoid robots from simulation to real-world environments, including system integration, safety protocols, and performance evaluation.